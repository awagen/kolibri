kolibri {
  actor-system = ${?KOLIBRI_ACTOR_SYSTEM_NAME}
  request {
    parallelism = 200
    parallelism = ${?REQUEST_PARALLELISM}
    useTracking = false
    useConnectionPoolFlow = false
    useConnectionPoolFlow = ${?USE_CONNECTION_POOL_FLOW}
    connection.pool.mode = "STANDARD"
    connection.pool.mode = ${?CONNECTION_POOL_MODE}
    connection.pool.moduleClass = ""
    connection.pool.moduleClass = ${?CONNECTION_POOL_MODULE_CLASS}
  }
  job {
   timeoutInSeconds = 1800
   processingCheckResourcesIntervalInMillis = 100
   runningTasksPerJobMaxCount = 20
   runningTasksPerJobMaxCount = ${?RUNNING_TASK_PER_JOB_MAX_COUNT}
   runningTasksPerJobDefaultCount = 3
   runningTasksPerJobDefaultCount = ${?RUNNING_TASK_PER_JOB_DEFAULT_COUNT}
   batchDistributionIntervalInMs = 1000
   batchMaxTimeToACKInMs = 1000
   resources {
   }
   tasks {
     waitForSynchronousTaskCompleteInMs = 100
   }
  }
  cluster {
    statusCheckTimeoutInMillis = 1000
    startClusterSingletonRouter = false
  }
  supervisor {
    maxNumOfRetries = 3
    maxNumOfRetriesWithinTimeInSeconds = 60
    houseKeepingIntervalInSeconds = 10
  }
  execution {
      runnableExecutionHouseKeepingIntervalInSeconds = 10
      runnableExecutionHouseKeepingIntervalInSeconds = ${?RUNNABLE_EXECUTION_HOUSEKEEPING_INTERVAL_IN_S}
      aggregatingActorHouseKeepingIntervalInSeconds = 10
      aggregatingActorHouseKeepingIntervalInSeconds = ${?AGGREGATING_ACTOR_HOUSEKEEPING_INTERVAL_IN_S}
      aggregatingActorStateSendingIntervalInSeconds = 2
      aggregatingActorStateSendingIntervalInSeconds = ${?AGGREGATING_ACTOR_STATE_SENDING_INTERVAL_IN_S}
      workManagerStateUpdateIntervalInSeconds = 2
      workManagerStateUpdateIntervalInSeconds = ${?WORK_MANAGER_STATE_UPDATE_INTERVAL_IN_S}
      workManagerReportBatchStateToJobManagerIntervalInSeconds = 2
      workManagerReportBatchStateToJobManagerIntervalInSeconds = ${?WORK_MANAGER_REPORT_BATCH_STATE_INTERVAL_IN_S}
      jobManagerCheckForCompletionIntervalInSeconds = 5
      jobManagerCheckForCompletionIntervalInSeconds = ${?JOB_MANAGER_CHECK_FOR_COMPLETION_INTERVAL_IN_S}
      # if useAggregatorBackpressure is true, the aggregator sends an ACK
      # message back to the producer. Within the ActorRunnables
      # this will enable using ask with a level of parallelism,
      # such that instead of just firing all results to the aggregator
      # this would use backpressure instead of filling the mailbox
      # of the AggregatingActor
      useAggregatorBackpressure = false
      useAggregatorBackpressure = ${?USE_AGGREGATOR_BACKPRESSURE}
      # the parallelism is only used if useAggregatorBackpressure is true
      aggregatorResultReceiveParallelism = 100
      aggregatorResultReceiveParallelism = ${?AGGREGATOR_RECEIVE_PARALLELISM}
      useResultElementGrouping = true
      useResultElementGrouping = ${?USE_RESULT_ELEMENT_GROUPING}
      resultElementGroupingCount = 500
      resultElementGroupingCount = ${?RESULT_ELEMENT_GROUPING_COUNT}
      resultElementGroupingIntervalInMs = 500
      resultElementGroupingIntervalInMs = ${?RESULT_ELEMENT_GROUPING_INTERVAL_IN_MS}
      resultElementGroupingParallelism = 1
      resultElementGroupingParallelism = ${?RESULT_ELEMENT_GROUPING_PARALLELISM}
      maxNrBatchRetries = 2
      maxNrBatchRetries = ${?MAX_NR_BATCH_RETRIES}
  }
  format {
    metricDocumentFormatType = "parameter"
    judgements {
      sourceType = "CSV"
      sourceType = ${?JUDGEMENT_FILE_SOURCE_TYPE}
      judgementKeyQueryAndProductDelimiter = "\u0000"
      jsonLines {
        queryPath = "query"
        queryPath = ${?JUDGEMENT_FILE_JSON_LINES_QUERY_PATH}
        productsPath = "products"
        productsPath = ${?JUDGEMENT_FILE_JSON_LINES_PRODUCTS_PATH}
        productIdSelector = "productId"
        productIdSelector = ${?JUDGEMENT_FILE_JSON_LINES_PRODUCT_ID_SELECTOR}
        judgementSelector = "score"
        judgementSelector = ${?JUDGEMENT_FILE_JSON_LINES_JUDGEMENT_SELECTOR}
        judgementValueTypeCast = "DOUBLE"
        judgementValueTypeCast = ${?JUDGEMENT_FILE_JSON_LINES_JUDGEMENT_VALUE_TYPE_CAST}
      }
      csv {
        judgementFileColumnDelimiter = "\u0000"
        judgementFileColumnDelimiter = ${?JUDGEMENT_FILE_COLUMN_DELIMITER}
        judgementFileIgnoreLessThanColumns = 3
        judgementFileIgnoreLessThanColumns = ${?JUDGEMENT_FILE_IGNORE_LESS_THAN_COLUMNS}
        judgementFileJudgementColumn = 2
        judgementFileJudgementColumn = ${?JUDGEMENT_FILE_JUDGEMENT_COLUMN}
        judgementFileSearchTermColumn = 0,
        judgementFileSearchTermColumn = ${?JUDGEMENT_FILE_SEARCH_TERM_COLUMN}
        judgementFileProductIdColumn = 1
        judgementFileProductIdColumn = ${?JUDGEMENT_FILE_PRODUCT_ID_COLUMN}
      }
    }
  }
  persistence {
    mode = ${?PERSISTENCE_MODE}
    moduleClass = ${?PERSISTENCE_MODULE_CLASS}
    directoryPathSeparator = "/"
    directoryPathSeparator = ${?DIRECTORY_PATH_SEPARATOR}
    csvColumnSeparator = "\t"
    csvColumnSeparator = ${?CSV_COLUMN_SEPARATOR}
    s3 {
      bucket = ${?AWS_S3_BUCKET}
      bucketPath = ${?AWS_S3_PATH}
      region = ${?AWS_S3_REGION}
    }
    gs {
      bucket = ${?GCP_GS_BUCKET}
      bucketPath = ${?GCP_GS_PATH}
      projectID = ${?GCP_GS_PROJECT_ID}
    }
    local {
      writeBasePath = ""
      writeBasePath = ${?LOCAL_STORAGE_WRITE_BASE_PATH}
      writeResultsSubPath = ""
      writeResultsSubPath = ${?LOCAL_STORAGE_WRITE_RESULTS_SUBPATH}
      readBasePath = ""
      readBasePath = ${?LOCAL_STORAGE_READ_BASE_PATH}
      resources {
        readBasePath = ""
        readBasePath = ${?LOCAL_RESOURCES_READ_BASE_PATH}
      }
    }
    templates {
      jobTemplatesPath = ${?JOB_TEMPLATES_PATH}
    }
  }
  internal {
    jobStatusRequestTimeoutInSeconds = 3
    analyzeTimeoutInSeconds = 30
  }
  ssl {
    useInsecureEngine = false
    useInsecureEngine = ${?USE_INSECURE_SSL_ENGINE}
  }
}

#custom dispatcher
kolibri-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    parallelism-min = ${?KOLIBRI_DISPATCHER_PARALLELISM_MIN}
    # Parallelism (threads) ... ceil(available processors * factor)
    # nr of threads used depends on nr of cores * factor with a resulting min and max defined with the
    # parallelism-min/parallelism-max settings
    parallelism-factor = 2.00
    parallelism-factor = ${?KOLIBRI_DISPATCHER_PARALLELISM_FACTOR}
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
    parallelism-max = ${?KOLIBRI_DISPATCHER_PARALLELISM_MAX}
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible
  throughput = 100
  throughput = ${?KOLIBRI_DISPATCHER_THROUGHPUT}
}

# overrides for akka-kryo-serialization lib (reference: https://github.com/altoo-ag/akka-kryo-serialization/blob/master/akka-kryo-serialization/src/main/resources/reference.conf)
akka-kryo-serialization {

    #max-buffer-size = 2147483639
    max-buffer-size = -1
    buffer-size = 4096
    # post-serialization-transformations = "lz4"
    post-serialization-transformations = "deflate"
    # kryo-reference-map = true

}

akka {
  # Put the following in your conf/logback.xml file:
  # <logger name="akka.actor" level="INFO" />
  # And then set the log-config-on-start value to true to debug the configuration.
  log-config-on-start = false

  cluster.downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

  actor {
    provider = "cluster"
    # https://doc.akka.io/docs/akka/current/serialization.html
    serializers {
      # https://doc.akka.io/docs/akka/current/serialization-jackson.html
      jackson-json = "akka.serialization.jackson.JacksonJsonSerializer"
      jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"
      proto = "akka.remote.serialization.ProtobufSerializer"
      # https://github.com/altoo-ag/akka-kryo-serialization
      kryo = "io.altoo.akka.serialization.kryo.KryoSerializer"
    }
    serialization-bindings {
      "com.google.protobuf.Message" = proto
      "de.awagen.kolibri.datatypes.io.KolibriSerializable" = kryo
    }
    allow-java-serialization = off
    # setting default dispatcher to limited resources (for actual processing refer to custom ones like kolibri-dispatcher)
    # e.g to limit kamon (metrics) resources or other ones that would just pick the default dispatcher
    default-dispatcher {
        type = Dispatcher
        executor = "fork-join-executor"
        fork-join-executor {
         parallelism-factor: 1.00,
         parallelism-factor: ${?DEFAULT_DISPATCHER_PARALLELISM_FACTOR},
         parallelism-max: 2,
         parallelism-max: ${?DEFAULT_DISPATCHER_PARALLELISM_MAX},
         parallelism-min: 1
         parallelism-min: ${?DEFAULT_DISPATCHER_PARALLELISM_MIN}
       }
    }
  }

  remote {
    log-remote-lifecycle-events = off
    enabled-transports = ["akka.remote.netty.tcp"]
    maximum-payload-bytes = 30000000 bytes # related to the OversizedPayloadException described below in the netty.tcp section
    netty.tcp {
      port = ${?CLUSTER_NODE_PORT}
      # allowing larger payload (otherwise larger messages (e.g aggregations) wont get through, resulting in messages like:
      # akka.remote.OversizedPayloadException: Discarding oversized payload sent to Actor[akka.tcp://KolibriAppSystem@172.33.1.1:40775/user/$b/solrExperimentManager-id1#-922313015]: max allowed size 128000 bytes, actual size of encoded class de.awagen.kolibri.searchopt.actors.work.serviceworker.MetricsAggregationWorker$ResultsProvidedEvent was 645757 bytes.
      message-frame-size =  30000000b
      send-buffer-size =  30000000b
      receive-buffer-size =  30000000b
      maximum-frame-size = 30000000b
    }
  }

  cluster {
    # defines role for the node using this config
    roles = ["httpserver", "compute"]
  }
}

#cluster-aware router
#detailed description of routing: https://doc.akka.io/docs/akka/current/routing.html?language=scala
#can define a Resizer in pool which automatically adjusts the number of instances (check if that works with
# router cluster-metrics-adaptive-group)
akka.actor.deployment {
  #see ClusterNode for creation of singleton creation with ClusterSingletonManager
  #singleton holds as soon as actor is created as singleton, routingActor is name of the respective actor and poolRouter the respective router created in the respective actor
  /singleton/routingActor/poolRouter {
    router = cluster-metrics-adaptive-pool
    metrics-selector = load
    # without defining the nr-of-instances and setting the max-nr-of-instances-per-node too high
    # can in high load situations cause non-routing of sent message, thus set reasonable values
    # TODO: haha no this was accidental since both max-nr and nr-of-instances had the same number and all
    # where placed on a single node , which was same as the one where jobmanager locates
    # nr-of-instances = 20
    cluster {
      enabled = on
      max-nr-of-instances-per-node = 10
      allow-local-routees = on
      use-roles = ["compute"]
    }
    pool-dispatcher {
      fork-join-executor.parallelism-min = 5
      fork-join-executor.parallelism-max = 20
    }
  }

  /poolRouter {
      router = cluster-metrics-adaptive-pool
      metrics-selector = load
      # without defining the nr-of-instances and setting the max-nr-of-instances-per-node too high
      # can in high load situations cause non-routing of sent message, thus set reasonable values
      # TODO: haha no this was accidental since both max-nr and nr-of-instances had the same number and all
      # where placed on a single node , which was same as the one where jobmanager locates
      # nr-of-instances = 20
      cluster {
        enabled = on
        max-nr-of-instances-per-node = 10
        allow-local-routees = on
        use-roles = ["compute"]
      }
      pool-dispatcher {
        fork-join-executor.parallelism-min = 5
        fork-join-executor.parallelism-max = 20
      }
    }
}

akka.extensions = [ "akka.cluster.metrics.ClusterMetricsExtension" ]

# added to ensure heartbeats (gossip protocol) are sent even if system under load
akka.cluster.use-dispatcher = cluster-dispatcher
cluster-dispatcher {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-factor = 1.0
    parallelism-min = 1
    parallelism-max = 1
  }
}


#connection pool settings
akka.http.host-connection-pool {
  # The maximum number of parallel connections that a connection pool to a
  # single host endpoint is allowed to establish. Must be greater than zero.
  max-connections = 100
  max-connections = ${?HTTP_CONNECTION_POOL_MAX_CONNECTIONS}

  # The minimum number of parallel connections that a pool should keep alive ("hot").
  # If the number of connections is falling below the given threshold, new ones are being spawned.
  # You can use this setting to build a hot pool of "always on" connections.
  # Default is 0, meaning there might be no active connection at given moment.
  # Keep in mind that `min-connections` should be smaller than `max-connections` or equal
  min-connections = 0

  # The maximum number of times failed requests are attempted again,
  # (if the request can be safely retried) before giving up and returning an error.
  # Set to zero to completely disable request retries.
  max-retries = 3
  max-retries = ${?HTTP_CONNECTION_POOL_MAX_RETRIES}

  # The maximum number of open requests accepted into the pool across all
  # materializations of any of its client flows.
  # Protects against (accidentally) overloading a single pool with too many client flow materializations.
  # Note that with N concurrent materializations the max number of open request in the pool
  # will never exceed N * max-connections * pipelining-limit.
  # Must be a power of 2 and > 0!
  max-open-requests = 512  #set quite high since its just a safeguard against too many connection pool flow materializations
  max-open-requests = ${?HTTP_CONNECTION_POOL_MAX_OPEN_REQUESTS}

  # The maximum number of requests that are dispatched to the target host in
  # batch-mode across a single connection (HTTP pipelining).
  # A setting of 1 disables HTTP pipelining, since only one request per
  # connection can be "in flight" at any time.
  # Set to higher values to enable HTTP pipelining.
  # This value must be > 0.
  # (Note that, independently of this setting, pipelining will never be done
  # on a connection that still has a non-idempotent request in flight.
  #
  # Before increasing this value, make sure you understand the effects of head-of-line blocking.
  # Using a connection pool, a request may be issued on a connection where a previous
  # long-running request hasn't finished yet. The response to the pipelined requests may then be stuck
  # behind the response of the long-running previous requests on the server. This may introduce an
  # unwanted "coupling" of run time between otherwise unrelated requests.
  #
  # See http://tools.ietf.org/html/rfc7230#section-6.3.2 for more info.)
  pipelining-limit = 1

  # The time after which an idle connection pool (without pending requests)
  # will automatically terminate itself. Set to `infinite` to completely disable idle timeouts.
  idle-timeout = 30 s

  # The pool implementation to use. Currently supported are:
  #  - legacy: the original 10.0.x pool implementation
  #  - new: the pool implementation that became the default in 10.1.x and will receive fixes and new features
  pool-implementation = new

  # The "new" pool implementation will fail a connection early and clear the slot if a response entity was not
  # subscribed during the given time period after the response was dispatched. In busy systems the timeout might be
  # too tight if a response is not picked up quick enough after it was dispatched by the pool.
  response-entity-subscription-timeout = 3.second
  response-entity-subscription-timeout = ${?HTTP_CONNECTION_POOL_SUBSCRIPTION_TIMEOUT}
}

akka.http.client {
  # The time period within which the TCP connecting process must be completed.
  connection-timeout = 3s
  connection-timeout = ${?HTTP_CLIENT_CONNECTION_TIMEOUT}

  # The time after which an idle connection will be automatically closed.
  # Set to `infinite` to completely disable idle timeouts.
  idle-timeout = 5s
  idle-timeout = ${?HTTP_CLIENT_IDLE_TIMEOUT}
}

#akka routing settings (akka-http section in : https://doc.akka.io/docs/akka-http/current/configuration.html)
akka.http.routing {
  # Maximum content length after applying a decoding directive.
  decode-max-size = 8m

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by FlowMaterialiser when creating Actors for IO operations.
  file-io-dispatcher = ${akka.stream.blocking-io-dispatcher}
}

akka.cluster.shutdown-after-unsuccessful-join-seed-nodes = 60s

#If you are running multiple clusters in the same JVM, set this 'on'
# akka.cluster.jmx.multi-mbeans-in-same-jvm = on
akka.cluster.jmx.multi-mbeans-in-same-jvm = off

#only logs first number of dead letters (not in total, per case)
akka.log-dead-letters = 10
# akka.log-dead-letters = on
# akka.log-dead-letters-during-shutdown = on
akka.log-dead-letters-during-shutdown = off

#need to create logger with "val log = Logging(system.eventStream, "my.nice.string") or other constructor"
akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
}